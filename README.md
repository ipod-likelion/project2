# NL2SQL Project

이 프로젝트는 자연어 질문을 SQL 쿼리로 변환하여 데이터베이스에서 답변을 찾는 NL2SQL 시스템입니다.
웹서버 데이터베이스 구축부터 모델 학습, 최종 추론까지의 과정을 포함합니다.

## 파일 구성 (Project Structure)

| 폴더/파일명 | 설명 |
|---|---|
| **final_execution_batched.ipynb** | **최종 결과물 (Final Task Execution)** <br>최종적으로 구축된 파이프라인을 통해 NL2SQL 태스크를 수행하는 노트북입니다. |
| **final_searcher_train.ipynb** | **Searcher 모델 학습** <br>검색(Retriever) 단계에서 사용되는 서쳐 모델을 학습하는 노트북입니다. |
| **final_train_retriever_reranker.ipynb** | **Retriever & Reranker 학습** <br>임베딩 및 재순위 모델을 학습하고 설정하는 노트북입니다. |
| **nl2sql/** | **웹서버 데이터베이스 (Web Server Database)** <br>시스템 구동을 위한 데이터베이스 관련 소스 코드 및 설정 파일이 포함된 폴더입니다. |
| **convert_data.py** | **데이터 변환 스크립트** <br>원시 데이터를 모델 학습에 적합한 형식으로 변환하는 스크립트입니다 공공데이터셋에 포함된 컨버터 스크립트를 수정해 사용했습니다. |

---

## 프로젝트 설명 (Methodology)

### 검색(Retriever): 왜 E5이고, 왜 '컬럼 단위'인가?

```python
# 코드: p_text = f"passage: {t_en}.{c_en[1]}({c_ko[1]})"
```

**전략: Bi-Encoder & Column-Level Indexing**

- **분석 (Why?):**

1. **테이블 vs 컬럼:** 기존 방식은 테이블 전체(스키마)를 임베딩합니다. 하지만 테이블 하나에 컬럼이 50개라면, 그중 질문과 관련 없는 48개는 **노이즈**가 됩니다. 우리는 **컬럼 하나하나를 독립된 문서**로 취급하여, 질문과 정확히 매칭되는 '바늘'을 찾도록 설계했습니다.

2. **Contrastive Learning:** 1 Epoch 만에 Loss가 0.02로 떨어진 것은, 모델이 (질문, 정답 컬럼)의 의미적 거리를 좁히고 (질문, 오답 컬럼)의 거리를 벌리는 학습을 완벽히 수행했기 때문입니다.

3. **속도:** E5는 문장을 벡터로 미리 변환해둘 수 있어, 5,700개 컬럼을 0.1초 안에 검색할 수 있습니다. (Matrix Multiplication)

→ **벡터 임베딩의 의의 (E5가 한 일: "단어를 좌표로 변환")**

컴퓨터는 "약국 주소"라는 한글을 이해하지 못합니다. 대신 **숫자의 나열(Vector)**은 기가 막히게 계산합니다. E5는 텍스트를 받아서 **768개의 숫자 리스트**로 바꿔줍니다.

- **입력:** query: 3층 약국 주소 알려줘
- **E5 처리:** (트랜스포머 연산 중...) ⚙️
- **출력 (임베딩 벡터):** [0.12, -0.88, 0.45, ..., 0.01] (총 768개의 숫자)

### 재순위(Reranker): 왜 KoELECTRA가 필요한가?

**선택 모델:** `monologg/koelectra-base-v3-discriminator`

- **전략: Cross-Encoder & Precision Filtering**
- **분석 (Why?):**

1. **벡터의 한계:** E5(검색기)는 "병원 주소"와 "약국 주소"를 벡터 공간상에서 매우 가깝게 배치합니다. 둘 다 '의료기관' + '주소'이기 때문입니다. 이로 인해 검색 단계에서는 오답이 많이 섞여 들어옵니다.

2. **정밀 채점:** KoELECTRA는 `[CLS] 질문 [SEP] 컬럼` 형태로 두 문장을 동시에 보면서 **"이 질문에 이 컬럼이 진짜 맞아?"**를 0~1점 사이 점수로 채점합니다.

3. **성과:** 이 단계 덕분에 상위 20개 중 섞여 있던 '비슷한 오답'들이 걸러지고, 진짜 정답 Top-5가 남게 되었습니다.

### 전략(Logic): 왜 'Top-1 Table'을 강제했나?

**전략: Deterministic Filtering (Top-1 Voting)**

- **분석 (Why?):**

1. **환각(Hallucination)의 원인:** LLM에게 여러 테이블(약국 정보, 주차장 정보 등)을 동시에 던져주면, LLM은 똑똑한 척을 하려다 **"약국 이름은 약국 테이블에서, 주차장 위치는 주차장 테이블에서 가져와서 JOIN하자"**는 잘못된 판단을 내립니다.

2. **인간의 개입:** 검색된 Top-5 컬럼들이 가리키는 테이블이 대다수(예: 4개)가 TB_PHARMACY라면, 사용자의 의도는 약국일 확률이 100%입니다.

3. **해결:** 알고리즘적으로 가장 유력한 **단 하나의 테이블**만 남기고 나머지는 버림으로써, LLM이 딴생각(JOIN)을 할 여지를 원천 봉쇄했습니다.

### 생성(Generator): 왜 SQLCoder이고, 프롬프트가 중요한가?

**선택 모델:** `defog/sqlcoder-7b-2`

- **전략: Domain-Specific Model & Dictator Prompting**
- **분석 (Why?):**

1. **범용 vs 전용:** Llama-3 같은 범용 모델은 말을 잘하지만 SQL 문법(특히 SQLite의 까다로운 날짜 함수 등)에 약합니다. SQLCoder는 SQL 생성에 특화되어 학습되었기에 스키마 이해도가 높습니다.

2. **독재자 프롬프트(Dictator Prompt):** 7B 모델은 파라미터 수가 적어 논리력이 부족할 수 있습니다. "생각해봐"라고 하기보다, **"이름은 DUTYNAME, 주소는 DUTYADDR, 요일은 DUTYTIME1"**이라고 정답지를 주입(Injection)하는 방식이 훨씬 효과적입니다.
