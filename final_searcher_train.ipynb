{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNZfn/VGvDROen9/wquNA9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6devr_uzLjZC"},"outputs":[],"source":["\n","import os\n","import json\n","import random\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm.auto import tqdm\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# =============================================================================\n","# [1] ì„¤ì •\n","# =============================================================================\n","WORK_DIR = \"/content/drive/MyDrive/P02_SemanticParsing\"\n","BASE_DIR = f\"{WORK_DIR}/nia\"\n","TRAIN_PATH = f\"{BASE_DIR}/train.json\"\n","TABLE_PATH = f\"{BASE_DIR}/tables.json\"\n","SAVE_DIR = f\"{WORK_DIR}/saved_models_final/schema_linker_e5\"\n","\n","MODEL_NAME = \"upskyy/e5-base-korean\"\n","BATCH_SIZE = 32\n","EPOCHS = 3\n","LR = 2e-5\n","MAX_LEN = 128\n","TEMPERATURE = 0.05\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"ğŸ”¥ Schema Linking í•™ìŠµ ì¬ì‹œì‘ (Fixed) (Device: {device})\")\n","\n","# =============================================================================\n","# [2] ë°ì´í„° ì „ì²˜ë¦¬ (ë²„ê·¸ ìˆ˜ì •ë¨ â˜…)\n","# =============================================================================\n","def load_data(train_path, table_path):\n","    with open(train_path, 'r') as f:\n","        train_data = json.load(f)\n","    with open(table_path, 'r') as f:\n","        table_list = json.load(f)\n","        tables = {t['db_id']: t for t in table_list}\n","    return train_data, tables\n","\n","def extract_gold_columns(sql_dict):\n","    \"\"\"\n","    Spider SQL í¬ë§·ì˜ ì¤‘ì²© êµ¬ì¡°ë¥¼ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•˜ì—¬ ì‚¬ìš©ëœ ì»¬ëŸ¼ ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","    \"\"\"\n","    used_cols = set()\n","\n","    # Helper: val_unitì—ì„œ ì»¬ëŸ¼ ì¶”ì¶œ ([unit_op, col_unit1, col_unit2])\n","    def get_col_from_val_unit(val_unit):\n","        if not val_unit: return\n","        # col_unit1: [agg, col_id, is_distinct]\n","        col_unit1 = val_unit[1]\n","        if col_unit1: used_cols.add(col_unit1[1])\n","        col_unit2 = val_unit[2]\n","        if col_unit2: used_cols.add(col_unit2[1])\n","\n","    # 1. SELECT ì ˆ: [is_distinct, [[agg, val_unit], ...]]\n","    if 'select' in sql_dict:\n","        select_info = sql_dict['select']\n","        # index 0ì€ bool(distinct), index 1ì´ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸\n","        if len(select_info) > 1 and isinstance(select_info[1], list):\n","            for agg, val_unit in select_info[1]:\n","                get_col_from_val_unit(val_unit)\n","\n","    # 2. WHERE ì ˆ: [[not, op, val_unit, val1, val2], ...] (ì¡°ê±´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸)\n","    if 'where' in sql_dict:\n","        for cond in sql_dict['where']:\n","            # ì¡°ê±´ì ˆì€ ë³´í†µ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì´ë©° ê¸¸ì´ê°€ 3 ì´ìƒ (AND/OR ë“± ì—°ê²°ì–´ ì œì™¸)\n","            if isinstance(cond, list) and len(cond) > 2:\n","                # index 2ê°€ val_unit\n","                get_col_from_val_unit(cond[2])\n","\n","    # 3. GROUP BY: [[col_unit...], ...]\n","    if 'groupBy' in sql_dict and sql_dict['groupBy']:\n","        for col_unit in sql_dict['groupBy']:\n","            if col_unit: used_cols.add(col_unit[1])\n","\n","    # 4. ORDER BY: [order, [val_unit, ...]]\n","    if 'orderBy' in sql_dict and sql_dict['orderBy']:\n","        order_info = sql_dict['orderBy']\n","        if len(order_info) > 1 and isinstance(order_info[1], list):\n","            for val_unit in order_info[1]:\n","                get_col_from_val_unit(val_unit)\n","\n","    # 5. HAVING\n","    if 'having' in sql_dict and sql_dict['having']:\n","        for cond in sql_dict['having']:\n","            if isinstance(cond, list) and len(cond) > 2:\n","                get_col_from_val_unit(cond[2])\n","\n","    # 0ë²ˆ ì¸ë±ìŠ¤(*) ì œì™¸\n","    if 0 in used_cols: used_cols.remove(0)\n","\n","    return list(used_cols)\n","\n","# =============================================================================\n","# [3] ë°ì´í„°ì…‹\n","# =============================================================================\n","class SchemaLinkingDataset(Dataset):\n","    def __init__(self, train_data, tables, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.pairs = []\n","\n","        print(\"ğŸ“‚ í•™ìŠµ ë°ì´í„° ìŒ ìƒì„± ì¤‘...\")\n","        for item in tqdm(train_data):\n","            db_id = item['db_id']\n","            question = item['question']\n","\n","            if db_id not in tables: continue\n","            table_info = tables[db_id]\n","\n","            # ë²„ê·¸ ìˆ˜ì •ëœ í•¨ìˆ˜ë¡œ ì •ë‹µ ì»¬ëŸ¼ ì¶”ì¶œ\n","            gold_col_indices = extract_gold_columns(item['sql'])\n","\n","            if not gold_col_indices: continue # ì •ë‹µ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ (*ë§Œ ìˆëŠ” ê²½ìš° ë“±)\n","\n","            for idx in gold_col_indices:\n","                # í…Œì´ë¸”/ì»¬ëŸ¼ ì •ë³´ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n","                t_names = table_info.get('table_names_original', [\"UNKNOWN\"])\n","                t_name_en = t_names[0] if isinstance(t_names, list) else str(t_names)\n","\n","                c_names_en = table_info['column_names_original']\n","                c_names_ko = table_info['column_names']\n","\n","                # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬ (ë°ì´í„°ì…‹ ì˜¤ë¥˜ ë°©ì§€)\n","                if idx >= len(c_names_en): continue\n","\n","                c_name_en_str = c_names_en[idx][1]\n","                c_name_ko_str = c_names_ko[idx][1]\n","\n","                # \"query: ...\" / \"passage: TB_NAME.COL_NAME(í•œê¸€ëª…)\"\n","                q_text = f\"query: {question}\"\n","                p_text = f\"passage: {t_name_en}.{c_name_en_str}({c_name_ko_str})\"\n","\n","                self.pairs.append((q_text, p_text))\n","\n","        print(f\"âœ… ì´ {len(self.pairs)}ê°œì˜ (ì§ˆë¬¸, ì •ë‹µì»¬ëŸ¼) ë°ì´í„° ìƒì„± ì™„ë£Œ!\")\n","\n","    def __len__(self): return len(self.pairs)\n","    def __getitem__(self, idx): return self.pairs[idx]\n","\n","def collate_fn(batch):\n","    queries = [b[0] for b in batch]\n","    passages = [b[1] for b in batch]\n","    tok_q = tokenizer(queries, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n","    tok_p = tokenizer(passages, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n","    return tok_q, tok_p\n","\n","# =============================================================================\n","# [4] ëª¨ë¸ & í•™ìŠµ\n","# =============================================================================\n","class E5Model(nn.Module):\n","    def __init__(self, model_name):\n","        super().__init__()\n","        self.backbone = AutoModel.from_pretrained(model_name)\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden = outputs.last_hidden_state\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n","        sum_embeddings = torch.sum(last_hidden * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        embeddings = sum_embeddings / sum_mask\n","        return torch.nn.functional.normalize(embeddings, p=2, dim=1)\n","\n","if __name__ == '__main__':\n","    train_data, tables = load_data(TRAIN_PATH, TABLE_PATH)\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","    # ë°ì´í„°ì…‹ ìƒì„± (ìˆ˜ì •ëœ ë¡œì§ ì ìš©)\n","    dataset = SchemaLinkingDataset(train_data, tables, tokenizer)\n","    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n","\n","    model = E5Model(MODEL_NAME).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n","    loss_fn = nn.CrossEntropyLoss()\n","    scaler = GradScaler()\n","\n","    print(\"\\nğŸš€ í•™ìŠµ ì‹œì‘...\")\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        total_loss = 0\n","        loop = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n","\n","        for tok_q, tok_p in loop:\n","            tok_q = {k: v.to(device) for k, v in tok_q.items()}\n","            tok_p = {k: v.to(device) for k, v in tok_p.items()}\n","\n","            optimizer.zero_grad()\n","            with autocast():\n","                q_emb = model(tok_q['input_ids'], tok_q['attention_mask'])\n","                p_emb = model(tok_p['input_ids'], tok_p['attention_mask'])\n","                sim_scores = torch.matmul(q_emb, p_emb.T) / TEMPERATURE\n","                labels = torch.arange(sim_scores.size(0)).to(device)\n","                loss = loss_fn(sim_scores, labels)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            total_loss += loss.item()\n","            loop.set_postfix(loss=f\"{loss.item():.4f}\")\n","\n","        save_path = f\"{SAVE_DIR}/epoch_{epoch+1}\"\n","        os.makedirs(save_path, exist_ok=True)\n","        model.backbone.save_pretrained(save_path)\n","        tokenizer.save_pretrained(save_path)\n","        print(f\"âœ¨ Epoch {epoch+1} ì €ì¥ ì™„ë£Œ\")\n","\n","    print(\"ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì´ì œ Top-5 ì»¬ëŸ¼ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")"]}]}